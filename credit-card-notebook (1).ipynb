{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7461488,"sourceType":"datasetVersion","datasetId":4342730}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-28T09:13:42.648501Z","iopub.execute_input":"2024-01-28T09:13:42.648891Z","iopub.status.idle":"2024-01-28T09:13:43.130498Z","shell.execute_reply.started":"2024-01-28T09:13:42.648858Z","shell.execute_reply":"2024-01-28T09:13:43.129313Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/credit-card/Creditcard_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/credit-card/Creditcard_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:43.132996Z","iopub.execute_input":"2024-01-28T09:13:43.133497Z","iopub.status.idle":"2024-01-28T09:13:43.166448Z","shell.execute_reply.started":"2024-01-28T09:13:43.133436Z","shell.execute_reply":"2024-01-28T09:13:43.165330Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:43.168220Z","iopub.execute_input":"2024-01-28T09:13:43.168953Z","iopub.status.idle":"2024-01-28T09:13:43.225629Z","shell.execute_reply.started":"2024-01-28T09:13:43.168909Z","shell.execute_reply":"2024-01-28T09:13:43.224408Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"     Time        V1        V2        V3        V4        V5        V6  \\\n0       0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n1       0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n2       1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n3       1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n4       2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n..    ...       ...       ...       ...       ...       ...       ...   \n767   575 -0.572263  0.731748  1.541254  0.150506  1.108974  0.372152   \n768   579 -1.296845 -0.511605  2.404726 -0.310762 -0.319551 -0.542842   \n769   579  1.214170  0.210481  0.484651  0.479768 -0.261955 -0.527039   \n770   580  1.267030 -0.071114  0.037680  0.512683  0.242392  0.705212   \n771   581  1.153758  0.132273  0.382969  1.405063 -0.224287 -0.197295   \n\n           V7        V8        V9  ...       V21       V22       V23  \\\n0    0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n1   -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n2    0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n3    0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n4    0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n..        ...       ...       ...  ...       ...       ...       ...   \n767  1.084879 -0.146329 -0.274447  ... -0.143508 -0.107582 -0.418263   \n768 -0.173310  0.260423 -1.202688  ... -0.071270 -0.161175  0.088496   \n769  0.021782 -0.106888 -0.037631  ... -0.224292 -0.594609  0.159877   \n770 -0.226582  0.109483  0.657565  ... -0.164468 -0.177225 -0.222918   \n771  0.020653  0.029260  0.412254  ... -0.107809 -0.125231 -0.057041   \n\n          V24       V25       V26       V27       V28  Amount  Class  \n0    0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n1   -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n2   -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n3   -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n4    0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n..        ...       ...       ...       ...       ...     ...    ...  \n767 -0.731029  0.877525 -0.364150 -0.177509 -0.256545   26.72      0  \n768  0.285390  0.281069 -0.370130  0.043410  0.092318   80.00      0  \n769  0.091873  0.140964  0.227406 -0.017389  0.016030    5.98      0  \n770 -1.245505  0.678360  0.525059  0.002920 -0.003333   12.36      0  \n771  0.073082  0.633977 -0.310685  0.033590  0.015250   13.79      0  \n\n[772 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>575</td>\n      <td>-0.572263</td>\n      <td>0.731748</td>\n      <td>1.541254</td>\n      <td>0.150506</td>\n      <td>1.108974</td>\n      <td>0.372152</td>\n      <td>1.084879</td>\n      <td>-0.146329</td>\n      <td>-0.274447</td>\n      <td>...</td>\n      <td>-0.143508</td>\n      <td>-0.107582</td>\n      <td>-0.418263</td>\n      <td>-0.731029</td>\n      <td>0.877525</td>\n      <td>-0.364150</td>\n      <td>-0.177509</td>\n      <td>-0.256545</td>\n      <td>26.72</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>768</th>\n      <td>579</td>\n      <td>-1.296845</td>\n      <td>-0.511605</td>\n      <td>2.404726</td>\n      <td>-0.310762</td>\n      <td>-0.319551</td>\n      <td>-0.542842</td>\n      <td>-0.173310</td>\n      <td>0.260423</td>\n      <td>-1.202688</td>\n      <td>...</td>\n      <td>-0.071270</td>\n      <td>-0.161175</td>\n      <td>0.088496</td>\n      <td>0.285390</td>\n      <td>0.281069</td>\n      <td>-0.370130</td>\n      <td>0.043410</td>\n      <td>0.092318</td>\n      <td>80.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>769</th>\n      <td>579</td>\n      <td>1.214170</td>\n      <td>0.210481</td>\n      <td>0.484651</td>\n      <td>0.479768</td>\n      <td>-0.261955</td>\n      <td>-0.527039</td>\n      <td>0.021782</td>\n      <td>-0.106888</td>\n      <td>-0.037631</td>\n      <td>...</td>\n      <td>-0.224292</td>\n      <td>-0.594609</td>\n      <td>0.159877</td>\n      <td>0.091873</td>\n      <td>0.140964</td>\n      <td>0.227406</td>\n      <td>-0.017389</td>\n      <td>0.016030</td>\n      <td>5.98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>770</th>\n      <td>580</td>\n      <td>1.267030</td>\n      <td>-0.071114</td>\n      <td>0.037680</td>\n      <td>0.512683</td>\n      <td>0.242392</td>\n      <td>0.705212</td>\n      <td>-0.226582</td>\n      <td>0.109483</td>\n      <td>0.657565</td>\n      <td>...</td>\n      <td>-0.164468</td>\n      <td>-0.177225</td>\n      <td>-0.222918</td>\n      <td>-1.245505</td>\n      <td>0.678360</td>\n      <td>0.525059</td>\n      <td>0.002920</td>\n      <td>-0.003333</td>\n      <td>12.36</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>771</th>\n      <td>581</td>\n      <td>1.153758</td>\n      <td>0.132273</td>\n      <td>0.382969</td>\n      <td>1.405063</td>\n      <td>-0.224287</td>\n      <td>-0.197295</td>\n      <td>0.020653</td>\n      <td>0.029260</td>\n      <td>0.412254</td>\n      <td>...</td>\n      <td>-0.107809</td>\n      <td>-0.125231</td>\n      <td>-0.057041</td>\n      <td>0.073082</td>\n      <td>0.633977</td>\n      <td>-0.310685</td>\n      <td>0.033590</td>\n      <td>0.015250</td>\n      <td>13.79</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>772 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Balancing the Dataset**","metadata":{}},{"cell_type":"code","source":"# Separating the class\nclass_count_0, class_count_1 = data['Class'].value_counts()\n\n# Separate class\nclass_0 = data[data['Class'] == 0]\nclass_1 = data[data['Class'] == 1]\nprint('class 0:', class_0.shape)\nprint('class 1:', class_1.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:43.228406Z","iopub.execute_input":"2024-01-28T09:13:43.228799Z","iopub.status.idle":"2024-01-28T09:13:43.244152Z","shell.execute_reply.started":"2024-01-28T09:13:43.228767Z","shell.execute_reply":"2024-01-28T09:13:43.242959Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"class 0: (763, 31)\nclass 1: (9, 31)\n","output_type":"stream"}]},{"cell_type":"code","source":"class_1_over = class_1.sample(class_count_0, replace=True)\n\ntest_over = pd.concat([class_1_over, class_0], axis=0)\n\nprint(\"total class of 1 and 0:\",test_over['Class'].value_counts())\ntest_over['Class'].value_counts().plot(kind='bar', title='count (target)')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:43.246158Z","iopub.execute_input":"2024-01-28T09:13:43.247141Z","iopub.status.idle":"2024-01-28T09:13:43.640775Z","shell.execute_reply.started":"2024-01-28T09:13:43.247083Z","shell.execute_reply":"2024-01-28T09:13:43.639535Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"total class of 1 and 0: Class\n1    763\n0    763\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<Axes: title={'center': 'count (target)'}, xlabel='Class'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import imblearn\nimport collections\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\n\nx = data.drop('Class', axis=1)\ny = data['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nros = RandomOverSampler(random_state=42)# fit predictor and target variable\nx_ros, y_ros = ros.fit_resample(X_train, y_train)\n\nprint('original dataset shape:', collections.Counter(y_train))\nprint('Resample dataset shape', collections.Counter(y_ros))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:43.642253Z","iopub.execute_input":"2024-01-28T09:13:43.642715Z","iopub.status.idle":"2024-01-28T09:13:45.952116Z","shell.execute_reply.started":"2024-01-28T09:13:43.642678Z","shell.execute_reply":"2024-01-28T09:13:45.949344Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"original dataset shape: Counter({0: 609, 1: 8})\nResample dataset shape Counter({0: 609, 1: 609})\n","output_type":"stream"}]},{"cell_type":"code","source":"# Merging the Balanced Data\ntrain_data = pd.concat([x_ros, y_ros], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:45.954222Z","iopub.execute_input":"2024-01-28T09:13:45.955204Z","iopub.status.idle":"2024-01-28T09:13:45.961158Z","shell.execute_reply.started":"2024-01-28T09:13:45.955166Z","shell.execute_reply":"2024-01-28T09:13:45.959943Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:45.963112Z","iopub.execute_input":"2024-01-28T09:13:45.963994Z","iopub.status.idle":"2024-01-28T09:13:46.018046Z","shell.execute_reply.started":"2024-01-28T09:13:45.963947Z","shell.execute_reply":"2024-01-28T09:13:46.017209Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n0      259 -0.363608  1.104008  1.300678  0.070314  0.075180 -1.010396   \n1       41  0.986063 -0.202965 -0.492768  0.407691  0.305660 -0.230529   \n2      417 -0.473731  0.697340  2.279600  1.359875  0.342429  1.392886   \n3      259  1.095067 -0.014393  1.408552  1.266546 -0.944751  0.029578   \n4      525 -0.755011 -0.517761  1.760091 -0.654206 -0.039143 -0.492847   \n...    ...       ...       ...       ...       ...       ...       ...   \n1213   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n1214     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n1215   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n1216   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n1217   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n0     0.683396 -0.073378 -0.401866  ... -0.261158 -0.685436 -0.013662   \n1     0.585028 -0.208225 -0.247503  ... -0.305874 -1.216555 -0.077602   \n2     0.289971  0.170677  0.578966  ... -0.462425 -0.486810 -0.235667   \n3    -0.598515  0.175291  0.485231  ...  0.013107  0.248009 -0.002564   \n4    -0.047345  0.118936  0.734444  ... -0.049608 -0.200904  0.268931   \n...        ...       ...       ...  ...       ...       ...       ...   \n1213  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n1214 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n1215  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n1216  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n1217  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n\n           V24       V25       V26       V27       V28  Amount  Class  \n0     0.336313 -0.157577  0.071699  0.245539  0.099037    0.99      0  \n1    -0.741341  0.286881  0.200347 -0.075203  0.027271  169.05      0  \n2    -0.726568  0.085981 -0.351095  0.289067 -0.043030    8.61      0  \n3     0.570100  0.387137 -0.442319  0.074531  0.032215    9.99      0  \n4     0.108087 -0.468660  0.729549 -0.017462  0.077163   79.54      0  \n...        ...       ...       ...       ...       ...     ...    ...  \n1213 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n1214 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n1215  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n1216  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n1217 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n\n[1218 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>259</td>\n      <td>-0.363608</td>\n      <td>1.104008</td>\n      <td>1.300678</td>\n      <td>0.070314</td>\n      <td>0.075180</td>\n      <td>-1.010396</td>\n      <td>0.683396</td>\n      <td>-0.073378</td>\n      <td>-0.401866</td>\n      <td>...</td>\n      <td>-0.261158</td>\n      <td>-0.685436</td>\n      <td>-0.013662</td>\n      <td>0.336313</td>\n      <td>-0.157577</td>\n      <td>0.071699</td>\n      <td>0.245539</td>\n      <td>0.099037</td>\n      <td>0.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41</td>\n      <td>0.986063</td>\n      <td>-0.202965</td>\n      <td>-0.492768</td>\n      <td>0.407691</td>\n      <td>0.305660</td>\n      <td>-0.230529</td>\n      <td>0.585028</td>\n      <td>-0.208225</td>\n      <td>-0.247503</td>\n      <td>...</td>\n      <td>-0.305874</td>\n      <td>-1.216555</td>\n      <td>-0.077602</td>\n      <td>-0.741341</td>\n      <td>0.286881</td>\n      <td>0.200347</td>\n      <td>-0.075203</td>\n      <td>0.027271</td>\n      <td>169.05</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>417</td>\n      <td>-0.473731</td>\n      <td>0.697340</td>\n      <td>2.279600</td>\n      <td>1.359875</td>\n      <td>0.342429</td>\n      <td>1.392886</td>\n      <td>0.289971</td>\n      <td>0.170677</td>\n      <td>0.578966</td>\n      <td>...</td>\n      <td>-0.462425</td>\n      <td>-0.486810</td>\n      <td>-0.235667</td>\n      <td>-0.726568</td>\n      <td>0.085981</td>\n      <td>-0.351095</td>\n      <td>0.289067</td>\n      <td>-0.043030</td>\n      <td>8.61</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>259</td>\n      <td>1.095067</td>\n      <td>-0.014393</td>\n      <td>1.408552</td>\n      <td>1.266546</td>\n      <td>-0.944751</td>\n      <td>0.029578</td>\n      <td>-0.598515</td>\n      <td>0.175291</td>\n      <td>0.485231</td>\n      <td>...</td>\n      <td>0.013107</td>\n      <td>0.248009</td>\n      <td>-0.002564</td>\n      <td>0.570100</td>\n      <td>0.387137</td>\n      <td>-0.442319</td>\n      <td>0.074531</td>\n      <td>0.032215</td>\n      <td>9.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>525</td>\n      <td>-0.755011</td>\n      <td>-0.517761</td>\n      <td>1.760091</td>\n      <td>-0.654206</td>\n      <td>-0.039143</td>\n      <td>-0.492847</td>\n      <td>-0.047345</td>\n      <td>0.118936</td>\n      <td>0.734444</td>\n      <td>...</td>\n      <td>-0.049608</td>\n      <td>-0.200904</td>\n      <td>0.268931</td>\n      <td>0.108087</td>\n      <td>-0.468660</td>\n      <td>0.729549</td>\n      <td>-0.017462</td>\n      <td>0.077163</td>\n      <td>79.54</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1213</th>\n      <td>539</td>\n      <td>-1.738582</td>\n      <td>0.052740</td>\n      <td>1.187057</td>\n      <td>-0.656652</td>\n      <td>0.920623</td>\n      <td>-0.291788</td>\n      <td>0.269083</td>\n      <td>0.140631</td>\n      <td>0.023464</td>\n      <td>...</td>\n      <td>-0.179545</td>\n      <td>-0.192036</td>\n      <td>-0.261879</td>\n      <td>-0.237477</td>\n      <td>-0.335040</td>\n      <td>0.240323</td>\n      <td>-0.345129</td>\n      <td>-0.383563</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1214</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1215</th>\n      <td>118</td>\n      <td>1.254914</td>\n      <td>0.350287</td>\n      <td>0.302488</td>\n      <td>0.693114</td>\n      <td>-0.371470</td>\n      <td>-1.070256</td>\n      <td>0.086781</td>\n      <td>-0.202836</td>\n      <td>0.035154</td>\n      <td>...</td>\n      <td>-0.287592</td>\n      <td>-0.832682</td>\n      <td>0.128083</td>\n      <td>0.339427</td>\n      <td>0.215944</td>\n      <td>0.094704</td>\n      <td>-0.023354</td>\n      <td>0.030892</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1216</th>\n      <td>574</td>\n      <td>1.257719</td>\n      <td>0.364739</td>\n      <td>0.306923</td>\n      <td>0.690638</td>\n      <td>-0.357792</td>\n      <td>-1.067481</td>\n      <td>0.094272</td>\n      <td>-0.210300</td>\n      <td>0.014455</td>\n      <td>...</td>\n      <td>-0.286856</td>\n      <td>-0.820658</td>\n      <td>0.127663</td>\n      <td>0.343128</td>\n      <td>0.221120</td>\n      <td>0.094391</td>\n      <td>-0.022189</td>\n      <td>0.030944</td>\n      <td>1.29</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>539</td>\n      <td>-1.738582</td>\n      <td>0.052740</td>\n      <td>1.187057</td>\n      <td>-0.656652</td>\n      <td>0.920623</td>\n      <td>-0.291788</td>\n      <td>0.269083</td>\n      <td>0.140631</td>\n      <td>0.023464</td>\n      <td>...</td>\n      <td>-0.179545</td>\n      <td>-0.192036</td>\n      <td>-0.261879</td>\n      <td>-0.237477</td>\n      <td>-0.335040</td>\n      <td>0.240323</td>\n      <td>-0.345129</td>\n      <td>-0.383563</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1218 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Simple Random Sampling\nz = 1.96\np = 0.5\ne = 0.05\n\n# Simple Sample size \nsim_ran_size = round((z**2 * p * (1 - p)) / (e**2)) # 384\n\nnp.random.seed = 42","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.019250Z","iopub.execute_input":"2024-01-28T09:13:46.020312Z","iopub.status.idle":"2024-01-28T09:13:46.025628Z","shell.execute_reply.started":"2024-01-28T09:13:46.020276Z","shell.execute_reply":"2024-01-28T09:13:46.024592Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sam = train_data.sample(n = sim_ran_size, random_state = 42)\n\nx = sam.drop('Class', axis=1)\ny = sam['Class']\nsam_1_x_train, sam_1_x_test, sam_1_y_train, sam_1_y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.030145Z","iopub.execute_input":"2024-01-28T09:13:46.030945Z","iopub.status.idle":"2024-01-28T09:13:46.051814Z","shell.execute_reply.started":"2024-01-28T09:13:46.030905Z","shell.execute_reply":"2024-01-28T09:13:46.050577Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sam_1_x_train","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.053195Z","iopub.execute_input":"2024-01-28T09:13:46.053576Z","iopub.status.idle":"2024-01-28T09:13:46.096591Z","shell.execute_reply.started":"2024-01-28T09:13:46.053543Z","shell.execute_reply":"2024-01-28T09:13:46.095593Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n184    192 -0.351287  1.183754  0.530794  0.543891  0.798441 -0.804472   \n100    304  1.049639  0.066437  0.059213  0.285469  0.635754  1.473074   \n323     12  1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069   \n914      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n462    257 -0.599318  0.887525  1.579214 -0.113728  0.304991 -0.182829   \n...    ...       ...       ...       ...       ...       ...       ...   \n1204   406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n628      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n561    256 -1.377245  0.714823  2.507513  0.865082 -0.290489  1.077328   \n1182   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n1168   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n\n            V7        V8        V9  ...       V20       V21       V22  \\\n184   1.563170 -0.935794  0.633634  ...  0.596401 -0.121755  0.668124   \n100  -0.376993  0.579045 -0.182240  ... -0.212006 -0.141137 -0.304712   \n323  -0.586057  0.189380  0.782333  ... -0.113910 -0.024612  0.196002   \n914  -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672   \n462   0.503722  0.241375 -0.722596  ... -0.088091 -0.137631 -0.524586   \n...        ...       ...       ...  ...       ...       ...       ...   \n1204 -2.537387  1.391657 -2.770089  ...  0.126911  0.517232 -0.035049   \n628  -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672   \n561   0.032507  0.510946  0.717788  ... -0.125335 -0.341853 -0.606731   \n1182  0.325574 -0.067794 -0.270953  ...  2.102339  0.661696  0.435477   \n1168  0.086781 -0.202836  0.035154  ... -0.079756 -0.287592 -0.832682   \n\n           V23       V24       V25       V26       V27       V28  Amount  \n184  -0.057380  0.301398 -0.730946 -0.538616 -0.037212 -0.479956    1.79  \n100   0.206105 -1.397547 -0.093121  0.227115  0.042340  0.001972    1.98  \n323   0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99  \n914   0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n462  -0.057729 -0.058980 -0.320342  0.089180  0.058979  0.098448    1.98  \n...        ...       ...       ...       ...       ...       ...     ...  \n1204 -0.465211  0.320198  0.044519  0.177840  0.261145 -0.143276    0.00  \n628   0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n561  -0.099740 -0.009123  0.328379 -0.506683 -0.032235  0.139841   13.53  \n1182  1.375966 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00  \n1168  0.128083  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69  \n\n[307 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>184</th>\n      <td>192</td>\n      <td>-0.351287</td>\n      <td>1.183754</td>\n      <td>0.530794</td>\n      <td>0.543891</td>\n      <td>0.798441</td>\n      <td>-0.804472</td>\n      <td>1.563170</td>\n      <td>-0.935794</td>\n      <td>0.633634</td>\n      <td>...</td>\n      <td>0.596401</td>\n      <td>-0.121755</td>\n      <td>0.668124</td>\n      <td>-0.057380</td>\n      <td>0.301398</td>\n      <td>-0.730946</td>\n      <td>-0.538616</td>\n      <td>-0.037212</td>\n      <td>-0.479956</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>304</td>\n      <td>1.049639</td>\n      <td>0.066437</td>\n      <td>0.059213</td>\n      <td>0.285469</td>\n      <td>0.635754</td>\n      <td>1.473074</td>\n      <td>-0.376993</td>\n      <td>0.579045</td>\n      <td>-0.182240</td>\n      <td>...</td>\n      <td>-0.212006</td>\n      <td>-0.141137</td>\n      <td>-0.304712</td>\n      <td>0.206105</td>\n      <td>-1.397547</td>\n      <td>-0.093121</td>\n      <td>0.227115</td>\n      <td>0.042340</td>\n      <td>0.001972</td>\n      <td>1.98</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>12</td>\n      <td>1.103215</td>\n      <td>-0.040296</td>\n      <td>1.267332</td>\n      <td>1.289091</td>\n      <td>-0.735997</td>\n      <td>0.288069</td>\n      <td>-0.586057</td>\n      <td>0.189380</td>\n      <td>0.782333</td>\n      <td>...</td>\n      <td>-0.113910</td>\n      <td>-0.024612</td>\n      <td>0.196002</td>\n      <td>0.013802</td>\n      <td>0.103758</td>\n      <td>0.364298</td>\n      <td>-0.382261</td>\n      <td>0.092809</td>\n      <td>0.037051</td>\n      <td>12.99</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n    </tr>\n    <tr>\n      <th>462</th>\n      <td>257</td>\n      <td>-0.599318</td>\n      <td>0.887525</td>\n      <td>1.579214</td>\n      <td>-0.113728</td>\n      <td>0.304991</td>\n      <td>-0.182829</td>\n      <td>0.503722</td>\n      <td>0.241375</td>\n      <td>-0.722596</td>\n      <td>...</td>\n      <td>-0.088091</td>\n      <td>-0.137631</td>\n      <td>-0.524586</td>\n      <td>-0.057729</td>\n      <td>-0.058980</td>\n      <td>-0.320342</td>\n      <td>0.089180</td>\n      <td>0.058979</td>\n      <td>0.098448</td>\n      <td>1.98</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1204</th>\n      <td>406</td>\n      <td>-2.312227</td>\n      <td>1.951992</td>\n      <td>-1.609851</td>\n      <td>3.997906</td>\n      <td>-0.522188</td>\n      <td>-1.426545</td>\n      <td>-2.537387</td>\n      <td>1.391657</td>\n      <td>-2.770089</td>\n      <td>...</td>\n      <td>0.126911</td>\n      <td>0.517232</td>\n      <td>-0.035049</td>\n      <td>-0.465211</td>\n      <td>0.320198</td>\n      <td>0.044519</td>\n      <td>0.177840</td>\n      <td>0.261145</td>\n      <td>-0.143276</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>628</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n    </tr>\n    <tr>\n      <th>561</th>\n      <td>256</td>\n      <td>-1.377245</td>\n      <td>0.714823</td>\n      <td>2.507513</td>\n      <td>0.865082</td>\n      <td>-0.290489</td>\n      <td>1.077328</td>\n      <td>0.032507</td>\n      <td>0.510946</td>\n      <td>0.717788</td>\n      <td>...</td>\n      <td>-0.125335</td>\n      <td>-0.341853</td>\n      <td>-0.606731</td>\n      <td>-0.099740</td>\n      <td>-0.009123</td>\n      <td>0.328379</td>\n      <td>-0.506683</td>\n      <td>-0.032235</td>\n      <td>0.139841</td>\n      <td>13.53</td>\n    </tr>\n    <tr>\n      <th>1182</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>2.102339</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n    </tr>\n    <tr>\n      <th>1168</th>\n      <td>118</td>\n      <td>1.254914</td>\n      <td>0.350287</td>\n      <td>0.302488</td>\n      <td>0.693114</td>\n      <td>-0.371470</td>\n      <td>-1.070256</td>\n      <td>0.086781</td>\n      <td>-0.202836</td>\n      <td>0.035154</td>\n      <td>...</td>\n      <td>-0.079756</td>\n      <td>-0.287592</td>\n      <td>-0.832682</td>\n      <td>0.128083</td>\n      <td>0.339427</td>\n      <td>0.215944</td>\n      <td>0.094704</td>\n      <td>-0.023354</td>\n      <td>0.030892</td>\n      <td>2.69</td>\n    </tr>\n  </tbody>\n</table>\n<p>307 rows × 30 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(random_state=42)\n\nmodel.fit(sam_1_x_train, sam_1_y_train)\n\ny_pred = model.predict(sam_1_x_test)\n\naccuracy = accuracy_score(sam_1_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.098291Z","iopub.execute_input":"2024-01-28T09:13:46.098693Z","iopub.status.idle":"2024-01-28T09:13:46.154698Z","shell.execute_reply.started":"2024-01-28T09:13:46.098659Z","shell.execute_reply":"2024-01-28T09:13:46.153565Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"85.71428571428571"},"metadata":{}}]},{"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(random_state=42, criterion=\"entropy\", max_depth=3)\n\nmodel.fit(sam_1_x_train, sam_1_y_train)\n\ny_pred = model.predict(sam_1_x_test)\n\naccuracy = accuracy_score(sam_1_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.156154Z","iopub.execute_input":"2024-01-28T09:13:46.156645Z","iopub.status.idle":"2024-01-28T09:13:46.179868Z","shell.execute_reply.started":"2024-01-28T09:13:46.156602Z","shell.execute_reply":"2024-01-28T09:13:46.178619Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"92.20779220779221"},"metadata":{}}]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42, max_depth = 4)\n\nmodel.fit(sam_1_x_train, sam_1_y_train)\n\ny_pred = model.predict(sam_1_x_test)\n\naccuracy = accuracy_score(sam_1_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.181198Z","iopub.execute_input":"2024-01-28T09:13:46.182041Z","iopub.status.idle":"2024-01-28T09:13:46.461398Z","shell.execute_reply.started":"2024-01-28T09:13:46.182005Z","shell.execute_reply":"2024-01-28T09:13:46.460233Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"98.7012987012987"},"metadata":{}}]},{"cell_type":"code","source":"# Support Vector Machines (SVM)\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='linear', random_state=42)\n\nmodel.fit(sam_1_x_train, sam_1_y_train)\n\ny_pred = model.predict(sam_1_x_test)\n\naccuracy = accuracy_score(sam_1_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:46.463007Z","iopub.execute_input":"2024-01-28T09:13:46.463376Z","iopub.status.idle":"2024-01-28T09:13:51.413990Z","shell.execute_reply.started":"2024-01-28T09:13:46.463342Z","shell.execute_reply":"2024-01-28T09:13:51.412846Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"84.4155844155844"},"metadata":{}}]},{"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\n\nmodel.fit(sam_1_x_train, sam_1_y_train)\n\ny_pred = model.predict(sam_1_x_test)\n\naccuracy = accuracy_score(sam_1_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:51.415794Z","iopub.execute_input":"2024-01-28T09:13:51.416534Z","iopub.status.idle":"2024-01-28T09:13:51.648278Z","shell.execute_reply.started":"2024-01-28T09:13:51.416468Z","shell.execute_reply":"2024-01-28T09:13:51.647398Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"98.7012987012987"},"metadata":{}}]},{"cell_type":"code","source":"# Stratified Sampling\n\nsam_2 = train_data.groupby('Class', group_keys=False).apply(lambda x: x.sample(400))\n\nprint(sam_2)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:51.649560Z","iopub.execute_input":"2024-01-28T09:13:51.650278Z","iopub.status.idle":"2024-01-28T09:13:51.697209Z","shell.execute_reply.started":"2024-01-28T09:13:51.650244Z","shell.execute_reply":"2024-01-28T09:13:51.696002Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"      Time        V1        V2        V3        V4        V5        V6  \\\n610     16  0.694885 -1.361819  1.029221  0.834159 -1.191209  1.309109   \n398     93 -1.640014 -0.479831  1.613630  2.997979 -3.803788  5.051812   \n604    335  1.295946  1.011835 -3.191860  0.471478  3.350241  2.432783   \n207    402 -0.160626 -0.064459  2.531072 -1.328268 -0.970430  0.185030   \n585    417 -0.791993  1.054007 -0.110012 -1.234186  2.751355  3.443064   \n...    ...       ...       ...       ...       ...       ...       ...   \n1153   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n1183   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n973    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n1060   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n1203   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n610  -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955 -0.050881   \n398   3.315254 -0.303785  0.671395  ... -0.367933  0.641891 -0.405779   \n604   0.189553  0.621734 -0.556702  ... -0.263248 -0.825372 -0.245258   \n207  -0.380184  0.078119  1.775956  ...  0.262124  1.340696 -0.253860   \n585   0.055313 -0.105660 -0.306074  ...  0.425088 -1.178829 -0.032154   \n...        ...       ...       ...  ...       ...       ...       ...   \n1153 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n1183  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n973   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n1060 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n1203  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n\n           V24       V25       V26       V27       V28  Amount  Class  \n610  -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71      0  \n398  -1.268700  0.494090  0.610967  0.608993 -0.410034  937.69      0  \n604   0.674749  1.010841 -0.279061  0.040542  0.090574    4.80      0  \n207   0.178601 -0.491121  0.244972  0.074829 -0.069924    0.01      0  \n585   0.928394 -0.128566  0.046299  0.069745 -0.037621    0.89      0  \n...        ...       ...       ...       ...       ...     ...    ...  \n1153 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n1183 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n973  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n1060 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n1203  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n\n[800 rows x 31 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"x = sam_2.drop('Class', axis=1)\ny = sam_2['Class']\nsam_2_x_train, sam_2_x_test, sam_2_y_train, sam_2_y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:51.699199Z","iopub.execute_input":"2024-01-28T09:13:51.700105Z","iopub.status.idle":"2024-01-28T09:13:51.714496Z","shell.execute_reply.started":"2024-01-28T09:13:51.700056Z","shell.execute_reply":"2024-01-28T09:13:51.713282Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(random_state=42)\n\nmodel.fit(sam_2_x_train, sam_2_y_train)\n\ny_pred = model.predict(sam_2_x_test)\n\naccuracy = accuracy_score(sam_2_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:51.716337Z","iopub.execute_input":"2024-01-28T09:13:51.717186Z","iopub.status.idle":"2024-01-28T09:13:51.911262Z","shell.execute_reply.started":"2024-01-28T09:13:51.717139Z","shell.execute_reply":"2024-01-28T09:13:51.909882Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"93.125"},"metadata":{}}]},{"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(random_state=42, criterion=\"entropy\", max_depth=3)\n\nmodel.fit(sam_2_x_train, sam_2_y_train)\n\ny_pred = model.predict(sam_2_x_test)\n\naccuracy = accuracy_score(sam_2_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:51.913143Z","iopub.execute_input":"2024-01-28T09:13:51.913780Z","iopub.status.idle":"2024-01-28T09:13:51.953087Z","shell.execute_reply.started":"2024-01-28T09:13:51.913735Z","shell.execute_reply":"2024-01-28T09:13:51.951255Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"96.875"},"metadata":{}}]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42, max_depth = 4, n_estimators = 500)\n\nmodel.fit(sam_2_x_train, sam_2_y_train)\n\ny_pred = model.predict(sam_2_x_test)\n\naccuracy = accuracy_score(sam_2_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:51.954655Z","iopub.execute_input":"2024-01-28T09:13:51.955812Z","iopub.status.idle":"2024-01-28T09:13:53.395186Z","shell.execute_reply.started":"2024-01-28T09:13:51.955766Z","shell.execute_reply":"2024-01-28T09:13:53.394051Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"100.0"},"metadata":{}}]},{"cell_type":"code","source":"# Support Vector Machines (SVM)\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='linear', random_state=42)\n\nmodel.fit(sam_2_x_train, sam_2_y_train)\n\ny_pred = model.predict(sam_2_x_test)\n\naccuracy = accuracy_score(sam_2_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:13:53.396494Z","iopub.execute_input":"2024-01-28T09:13:53.396862Z","iopub.status.idle":"2024-01-28T09:14:49.198535Z","shell.execute_reply.started":"2024-01-28T09:13:53.396831Z","shell.execute_reply":"2024-01-28T09:14:49.195453Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\n\nmodel.fit(sam_2_x_train, sam_2_y_train)\n\ny_pred = model.predict(sam_2_x_test)\n\naccuracy = accuracy_score(sam_2_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.200427Z","iopub.status.idle":"2024-01-28T09:14:49.201177Z","shell.execute_reply.started":"2024-01-28T09:14:49.200880Z","shell.execute_reply":"2024-01-28T09:14:49.200911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cluster Sampling\ndef get_clustered_Sample(df, n_per_cluster, num_select_clusters):\n    N = len(df)\n    K = int(N/n_per_cluster)\n    data = None\n    for k in range(K):\n        sample_k = df.sample(n_per_cluster)\n        sample_k[\"cluster\"] = np.repeat(k,len(sample_k))\n        df = df.drop(index = sample_k.index)\n        data = pd.concat([data,sample_k],axis = 0)\n\n    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)\n    samples = data[data.cluster.isin(random_chosen_clusters)]\n    return(samples)\n\nsam_3 = get_clustered_Sample(df = train_data, n_per_cluster = 100, num_select_clusters = 2)\nsam_3","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.204908Z","iopub.status.idle":"2024-01-28T09:14:49.206367Z","shell.execute_reply.started":"2024-01-28T09:14:49.205991Z","shell.execute_reply":"2024-01-28T09:14:49.206030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = sam_3.drop('Class', axis=1)\ny = sam_3['Class']\nsam_3_x_train, sam_3_x_test, sam_3_y_train, sam_3_y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.208692Z","iopub.status.idle":"2024-01-28T09:14:49.209283Z","shell.execute_reply.started":"2024-01-28T09:14:49.209034Z","shell.execute_reply":"2024-01-28T09:14:49.209058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(random_state=42)\n\nmodel.fit(sam_3_x_train, sam_3_y_train)\n\ny_pred = model.predict(sam_3_x_test)\n\naccuracy = accuracy_score(sam_3_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.211176Z","iopub.status.idle":"2024-01-28T09:14:49.211780Z","shell.execute_reply.started":"2024-01-28T09:14:49.211534Z","shell.execute_reply":"2024-01-28T09:14:49.211561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(random_state=42, criterion=\"entropy\", max_depth=3)\n\nmodel.fit(sam_3_x_train, sam_3_y_train)\n\ny_pred = model.predict(sam_3_x_test)\n\naccuracy = accuracy_score(sam_3_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.212810Z","iopub.status.idle":"2024-01-28T09:14:49.213249Z","shell.execute_reply.started":"2024-01-28T09:14:49.213052Z","shell.execute_reply":"2024-01-28T09:14:49.213071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42, max_depth = 4)\n\nmodel.fit(sam_3_x_train, sam_3_y_train)\n\ny_pred = model.predict(sam_3_x_test)\n\naccuracy = accuracy_score(sam_3_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.214541Z","iopub.status.idle":"2024-01-28T09:14:49.215068Z","shell.execute_reply.started":"2024-01-28T09:14:49.214841Z","shell.execute_reply":"2024-01-28T09:14:49.214863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machines (SVM)\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='linear', random_state=42)\n\nmodel.fit(sam_3_x_train, sam_3_y_train)\n\ny_pred = model.predict(sam_3_x_test)\n\naccuracy = accuracy_score(sam_3_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.218517Z","iopub.status.idle":"2024-01-28T09:14:49.219203Z","shell.execute_reply.started":"2024-01-28T09:14:49.218863Z","shell.execute_reply":"2024-01-28T09:14:49.218894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\n\nmodel.fit(sam_3_x_train, sam_3_y_train)\n\ny_pred = model.predict(sam_3_x_test)\n\naccuracy = accuracy_score(sam_3_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.220593Z","iopub.status.idle":"2024-01-28T09:14:49.221198Z","shell.execute_reply.started":"2024-01-28T09:14:49.220899Z","shell.execute_reply":"2024-01-28T09:14:49.220928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Systematic Sampling\n\ndef systematic_sampling(df, step):\n    indexes = np.arange(0, len(df), step=step)\n    systematic_sample = df.iloc[indexes]\n    return systematic_sample\n \nsam_4 = systematic_sampling(train_data, 3)\nsam_4","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.223574Z","iopub.status.idle":"2024-01-28T09:14:49.224167Z","shell.execute_reply.started":"2024-01-28T09:14:49.223924Z","shell.execute_reply":"2024-01-28T09:14:49.223947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = sam_4.drop('Class', axis=1)\ny = sam_4['Class']\nsam_4_x_train, sam_4_x_test, sam_4_y_train, sam_4_y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.225948Z","iopub.status.idle":"2024-01-28T09:14:49.226521Z","shell.execute_reply.started":"2024-01-28T09:14:49.226258Z","shell.execute_reply":"2024-01-28T09:14:49.226281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(random_state=42)\n\nmodel.fit(sam_4_x_train, sam_4_y_train)\n\ny_pred = model.predict(sam_4_x_test)\n\naccuracy = accuracy_score(sam_4_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.228652Z","iopub.status.idle":"2024-01-28T09:14:49.229556Z","shell.execute_reply.started":"2024-01-28T09:14:49.228968Z","shell.execute_reply":"2024-01-28T09:14:49.228991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(random_state=42, criterion=\"entropy\", max_depth=5)\n\nmodel.fit(sam_4_x_train, sam_4_y_train)\n\ny_pred = model.predict(sam_4_x_test)\n\naccuracy = accuracy_score(sam_4_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.230763Z","iopub.status.idle":"2024-01-28T09:14:49.231228Z","shell.execute_reply.started":"2024-01-28T09:14:49.231009Z","shell.execute_reply":"2024-01-28T09:14:49.231032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42, max_depth = 4)\n\nmodel.fit(sam_4_x_train, sam_4_y_train)\n\ny_pred = model.predict(sam_4_x_test)\n\naccuracy = accuracy_score(sam_4_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.232323Z","iopub.status.idle":"2024-01-28T09:14:49.233164Z","shell.execute_reply.started":"2024-01-28T09:14:49.232715Z","shell.execute_reply":"2024-01-28T09:14:49.232892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint('Training Accuracy : ', \n      metrics.accuracy_score(sam_4_y_train,\n                             model.predict(sam_4_x_train))*100)\nprint('Validation Accuracy : ', \n      metrics.accuracy_score(sam_4_y_test, \n                             model.predict(sam_4_x_test))*100)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.234711Z","iopub.status.idle":"2024-01-28T09:14:49.235284Z","shell.execute_reply.started":"2024-01-28T09:14:49.235025Z","shell.execute_reply":"2024-01-28T09:14:49.235049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machines (SVM)\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='linear', random_state=42)\n\nmodel.fit(sam_4_x_train, sam_4_y_train)\n\ny_pred = model.predict(sam_4_x_test)\n\naccuracy = accuracy_score(sam_4_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.237395Z","iopub.status.idle":"2024-01-28T09:14:49.238072Z","shell.execute_reply.started":"2024-01-28T09:14:49.237790Z","shell.execute_reply":"2024-01-28T09:14:49.237817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\n\nmodel.fit(sam_4_x_train, sam_4_y_train)\n\ny_pred = model.predict(sam_4_x_test)\n\naccuracy = accuracy_score(sam_4_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:14:49.239471Z","iopub.status.idle":"2024-01-28T09:14:49.240329Z","shell.execute_reply.started":"2024-01-28T09:14:49.239930Z","shell.execute_reply":"2024-01-28T09:14:49.239977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bootstrap Sampling\nsam_5 = train_data.sample(n=550, replace=True)\nsam_5","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:34:23.621180Z","iopub.execute_input":"2024-01-28T09:34:23.622128Z","iopub.status.idle":"2024-01-28T09:34:23.678422Z","shell.execute_reply.started":"2024-01-28T09:34:23.622077Z","shell.execute_reply":"2024-01-28T09:34:23.675586Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n1182   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n1146   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n625      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n800    406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n1198   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n...    ...       ...       ...       ...       ...       ...       ...   \n466    528  1.358535 -0.443057  0.700476 -0.734666 -1.138176 -0.830189   \n575    423  1.161123  0.654050  0.211966  2.521864  0.156582 -0.583461   \n401    411 -0.566531 -0.057728 -0.825121 -1.040222  1.059728  3.811274   \n472     55 -4.575093 -4.429184  3.402585  0.903915  3.002224 -0.491078   \n915    539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n1182  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n1146  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n625  -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n800  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n1198  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n...        ...       ...       ...  ...       ...       ...       ...   \n466  -0.637162 -0.074408 -1.078981  ... -0.031771 -0.219789  0.147941   \n575   0.442847 -0.120914 -0.959401  ...  0.025278  0.046085 -0.025789   \n401   0.066048  1.280204 -0.504435  ...  0.208875 -0.046195  0.567673   \n472  -2.705393  0.666451  1.922216  ... -0.047365  0.853360 -0.971600   \n915   0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n\n           V24       V25       V26       V27       V28  Amount  Class  \n1182 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n1146 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n625  -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n800   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n1198  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n...        ...       ...       ...       ...       ...     ...    ...  \n466   0.517099  0.186110 -0.510075  0.009997  0.014218    3.71      0  \n575   0.377012  0.585970  0.080868 -0.028148  0.008323    3.75      0  \n401   1.039253 -0.371477  0.997043 -0.198837 -0.019047  235.70      0  \n472  -0.114862  0.408300 -0.304576  0.547785 -0.456297  200.01      0  \n915  -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n\n[550 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1182</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1146</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>625</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>800</th>\n      <td>406</td>\n      <td>-2.312227</td>\n      <td>1.951992</td>\n      <td>-1.609851</td>\n      <td>3.997906</td>\n      <td>-0.522188</td>\n      <td>-1.426545</td>\n      <td>-2.537387</td>\n      <td>1.391657</td>\n      <td>-2.770089</td>\n      <td>...</td>\n      <td>0.517232</td>\n      <td>-0.035049</td>\n      <td>-0.465211</td>\n      <td>0.320198</td>\n      <td>0.044519</td>\n      <td>0.177840</td>\n      <td>0.261145</td>\n      <td>-0.143276</td>\n      <td>0.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>484</td>\n      <td>-0.928088</td>\n      <td>0.398194</td>\n      <td>1.741131</td>\n      <td>0.182673</td>\n      <td>0.966387</td>\n      <td>-0.901004</td>\n      <td>0.879016</td>\n      <td>-0.156590</td>\n      <td>-0.142117</td>\n      <td>...</td>\n      <td>0.066353</td>\n      <td>0.281378</td>\n      <td>-0.257966</td>\n      <td>0.385384</td>\n      <td>0.391117</td>\n      <td>-0.453853</td>\n      <td>-0.104448</td>\n      <td>-0.125765</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>466</th>\n      <td>528</td>\n      <td>1.358535</td>\n      <td>-0.443057</td>\n      <td>0.700476</td>\n      <td>-0.734666</td>\n      <td>-1.138176</td>\n      <td>-0.830189</td>\n      <td>-0.637162</td>\n      <td>-0.074408</td>\n      <td>-1.078981</td>\n      <td>...</td>\n      <td>-0.031771</td>\n      <td>-0.219789</td>\n      <td>0.147941</td>\n      <td>0.517099</td>\n      <td>0.186110</td>\n      <td>-0.510075</td>\n      <td>0.009997</td>\n      <td>0.014218</td>\n      <td>3.71</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>423</td>\n      <td>1.161123</td>\n      <td>0.654050</td>\n      <td>0.211966</td>\n      <td>2.521864</td>\n      <td>0.156582</td>\n      <td>-0.583461</td>\n      <td>0.442847</td>\n      <td>-0.120914</td>\n      <td>-0.959401</td>\n      <td>...</td>\n      <td>0.025278</td>\n      <td>0.046085</td>\n      <td>-0.025789</td>\n      <td>0.377012</td>\n      <td>0.585970</td>\n      <td>0.080868</td>\n      <td>-0.028148</td>\n      <td>0.008323</td>\n      <td>3.75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>411</td>\n      <td>-0.566531</td>\n      <td>-0.057728</td>\n      <td>-0.825121</td>\n      <td>-1.040222</td>\n      <td>1.059728</td>\n      <td>3.811274</td>\n      <td>0.066048</td>\n      <td>1.280204</td>\n      <td>-0.504435</td>\n      <td>...</td>\n      <td>0.208875</td>\n      <td>-0.046195</td>\n      <td>0.567673</td>\n      <td>1.039253</td>\n      <td>-0.371477</td>\n      <td>0.997043</td>\n      <td>-0.198837</td>\n      <td>-0.019047</td>\n      <td>235.70</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>472</th>\n      <td>55</td>\n      <td>-4.575093</td>\n      <td>-4.429184</td>\n      <td>3.402585</td>\n      <td>0.903915</td>\n      <td>3.002224</td>\n      <td>-0.491078</td>\n      <td>-2.705393</td>\n      <td>0.666451</td>\n      <td>1.922216</td>\n      <td>...</td>\n      <td>-0.047365</td>\n      <td>0.853360</td>\n      <td>-0.971600</td>\n      <td>-0.114862</td>\n      <td>0.408300</td>\n      <td>-0.304576</td>\n      <td>0.547785</td>\n      <td>-0.456297</td>\n      <td>200.01</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>539</td>\n      <td>-1.738582</td>\n      <td>0.052740</td>\n      <td>1.187057</td>\n      <td>-0.656652</td>\n      <td>0.920623</td>\n      <td>-0.291788</td>\n      <td>0.269083</td>\n      <td>0.140631</td>\n      <td>0.023464</td>\n      <td>...</td>\n      <td>-0.179545</td>\n      <td>-0.192036</td>\n      <td>-0.261879</td>\n      <td>-0.237477</td>\n      <td>-0.335040</td>\n      <td>0.240323</td>\n      <td>-0.345129</td>\n      <td>-0.383563</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>550 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x = sam_5.drop('Class', axis=1)\ny = sam_5['Class']\nsam_5_x_train, sam_5_x_test, sam_5_y_train, sam_5_y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:35:00.988248Z","iopub.execute_input":"2024-01-28T09:35:00.988688Z","iopub.status.idle":"2024-01-28T09:35:01.004135Z","shell.execute_reply.started":"2024-01-28T09:35:00.988653Z","shell.execute_reply":"2024-01-28T09:35:01.001965Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(random_state=42)\n\nmodel.fit(sam_5_x_train, sam_5_y_train)\n\ny_pred = model.predict(sam_5_x_test)\n\naccuracy = accuracy_score(sam_5_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:36:47.775052Z","iopub.execute_input":"2024-01-28T09:36:47.775678Z","iopub.status.idle":"2024-01-28T09:36:47.855736Z","shell.execute_reply.started":"2024-01-28T09:36:47.775639Z","shell.execute_reply":"2024-01-28T09:36:47.854453Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"93.63636363636364"},"metadata":{}}]},{"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(random_state=42, criterion=\"entropy\", max_depth=5)\n\nmodel.fit(sam_5_x_train, sam_5_y_train)\n\ny_pred = model.predict(sam_5_x_test)\n\naccuracy = accuracy_score(sam_5_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:36:57.239284Z","iopub.execute_input":"2024-01-28T09:36:57.240513Z","iopub.status.idle":"2024-01-28T09:36:57.267414Z","shell.execute_reply.started":"2024-01-28T09:36:57.240434Z","shell.execute_reply":"2024-01-28T09:36:57.265660Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"99.0909090909091"},"metadata":{}}]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42, max_depth = 4)\n\nmodel.fit(sam_5_x_train, sam_5_y_train)\n\ny_pred = model.predict(sam_5_x_test)\n\naccuracy = accuracy_score(sam_5_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:37:08.621577Z","iopub.execute_input":"2024-01-28T09:37:08.622260Z","iopub.status.idle":"2024-01-28T09:37:08.926327Z","shell.execute_reply.started":"2024-01-28T09:37:08.622213Z","shell.execute_reply":"2024-01-28T09:37:08.925099Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"100.0"},"metadata":{}}]},{"cell_type":"code","source":"# Support Vector Machines (SVM)\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='linear', random_state=42)\n\nmodel.fit(sam_5_x_train, sam_5_y_train)\n\ny_pred = model.predict(sam_5_x_test)\n\naccuracy = accuracy_score(sam_5_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:37:32.840745Z","iopub.execute_input":"2024-01-28T09:37:32.841719Z","iopub.status.idle":"2024-01-28T09:37:51.843733Z","shell.execute_reply.started":"2024-01-28T09:37:32.841665Z","shell.execute_reply":"2024-01-28T09:37:51.842070Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"96.36363636363636"},"metadata":{}}]},{"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\n\nmodel.fit(sam_5_x_train, sam_5_y_train)\n\ny_pred = model.predict(sam_5_x_test)\n\naccuracy = accuracy_score(sam_5_y_test, y_pred)\naccuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:37:29.159615Z","iopub.execute_input":"2024-01-28T09:37:29.160517Z","iopub.status.idle":"2024-01-28T09:37:29.195920Z","shell.execute_reply.started":"2024-01-28T09:37:29.160446Z","shell.execute_reply":"2024-01-28T09:37:29.194725Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"99.0909090909091"},"metadata":{}}]}]}